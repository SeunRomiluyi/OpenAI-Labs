{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "73b2428d",
      "metadata": {},
      "source": [
        "# Analyzing the Results from an FFModel Experiment\n",
        "\n",
        "This is a template notebook that extracts evaluation metrics from your FFModel experiments outputs.\n",
        "This is meant as an example to showcase how to analyze the experiment output, use this as a starting point for you to analyze your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f622651e",
      "metadata": {},
      "source": [
        "## Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c0cfcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Make sure you append the root of your repo.\n",
        "root_path = os.path.abspath(os.path.join(\"../..\"))\n",
        "\n",
        "if root_path not in sys.path:\n",
        "    sys.path.append(root_path)\n",
        "\n",
        "import pandas as pd\n",
        "from utilities import result_analysis_utils as ra\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_context(\"talk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c373fe",
      "metadata": {},
      "source": [
        "## Result Analysis\n",
        "\n",
        "The following configs are needed to run this notebook:\n",
        "\n",
        "- `experiments_outputs_path` the output folder path that holds the outputs from your experiments.\n",
        "- `metrics` is an array of metrics to extract and plot. If its `None`, then all of your metrics will be extracted/plotted.To ensure uniqueness among metrics produced from different components, the metric name needs to be `<component module name>_<metric name>`.\n",
        "  - For example, if you are producing a `syntax-valid` metric from `components.evaluators.python_syntax` the metric name would be `python_syntax.syntax-valid`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41070307",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiments_outputs_path = \"\"\n",
        "metrics = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79d1b28",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "The following functions will be used to extract and plot the metrics from your experiments outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1e8679",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_avg_scores(df, metrics=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Function used to compute average scores acorss each metric for each respective function\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): Dataframe consisting of metrics computed for each experiment\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame: Output dataframe consisting of avergae scores for each metric for each experiment\n",
        "    \"\"\"\n",
        "\n",
        "    dfs = []\n",
        "    experiments = []\n",
        "\n",
        "    # get all unique tags and generate metrics df\n",
        "    experiment_tags = df[\"experiment\"].unique()\n",
        "    metrics_df = ra.extract_metrics(df, metrics=metrics)\n",
        "\n",
        "    for tag in experiment_tags:\n",
        "        sub_df = metrics_df[metrics_df[\"experiment\"] == tag]\n",
        "        sub_df = sub_df.drop(\"experiment\", axis=1)\n",
        "        scores = pd.DataFrame(sub_df.mean(axis=0)).transpose()\n",
        "        scores.insert(0, \"experiment\", tag)\n",
        "\n",
        "        dfs.append(scores)\n",
        "\n",
        "    # concatenate dfs to single df\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    # df.to_csv(\"avg_scores.csv\", index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069e3982",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_experiment_results(df):\n",
        "    \"\"\"\n",
        "    Function used to plot experiment results for comparison across each experiment\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): Input Metrics Dataframe containing output from experiments\n",
        "    \"\"\"\n",
        "\n",
        "    columns = df.columns\n",
        "    column_names = {}\n",
        "\n",
        "    for column in columns:\n",
        "        column_names[column] = column.replace(\"_\", \" \").upper()\n",
        "\n",
        "    df.rename(columns=column_names, inplace=True)\n",
        "\n",
        "    # melt observations into columns\n",
        "    df = df.melt(id_vars=[\"EXPERIMENT\"], value_name=\"Metric\")\n",
        "\n",
        "    # Actual plotting\n",
        "    grid = sns.FacetGrid(\n",
        "        df, col=\"variable\", sharex=False, col_wrap=3, height=10, aspect=1\n",
        "    )\n",
        "    grid.map_dataframe(sns.boxplot, x=\"Metric\", y=\"EXPERIMENT\", data=df, palette=\"Set2\")\n",
        "    grid.set_titles(\"{col_name}\")\n",
        "    grid.set_ylabels(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d164687",
      "metadata": {},
      "source": [
        "## Read in Output Files from an Input Directory for Main Evaluation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb758172",
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in dataframe\n",
        "df = ra.load_experiments_outputs(experiments_outputs_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4d32d4",
      "metadata": {},
      "source": [
        "## Generate DataFrame Consisting of Metrics Computed from Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56636313",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df = ra.extract_metrics(df, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3616e275",
      "metadata": {},
      "source": [
        "## Generate Average Scores for each Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6677714b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate results statics for each experiment\n",
        "avg_scores = generate_avg_scores(df, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d9579d",
      "metadata": {},
      "source": [
        "## Plot Results for Comparison Across Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ed1062",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot results\n",
        "plot_experiment_results(metrics_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
