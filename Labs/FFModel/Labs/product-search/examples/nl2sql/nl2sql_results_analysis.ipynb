{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87e21e9f",
      "metadata": {},
      "source": [
        "# Experiment analysis\n",
        "\n",
        "This notebook compares the results of two experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9afa50b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c0cfcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_context(\"talk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a459c13",
      "metadata": {},
      "source": [
        "## Configs\n",
        "\n",
        "Configure these values to analyze your results. This notebook will compare all the output files in the given path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8683bdb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The experiment name where the output files are expected to exist.\n",
        "experiment_name = \"nl2sql\"\n",
        "\n",
        "# The full path to the output files.\n",
        "experiment_outputs_path = os.path.join(\"outputs\", experiment_name)\n",
        "\n",
        "# Set to True to save the figures, False otherwise.\n",
        "save_figures = True\n",
        "\n",
        "# The path to save the figures to\n",
        "figures_output_path = \"figures\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de07643",
      "metadata": {},
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11c2bce",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExperimentOutput:\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.data = []\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.path, \"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                self.data.append(json.loads(line))\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data\n",
        "\n",
        "    def get_error_points(self):\n",
        "        return [d for d in self.data if d[\"error\"]]\n",
        "\n",
        "    def to_dataframe(self) -> pd.DataFrame:\n",
        "        pass\n",
        "\n",
        "    def to_metric_dataframe(\n",
        "        self, fill_value=0.0, included_metrics=None\n",
        "    ) -> pd.DataFrame:\n",
        "        evaluation_metrics: List[Dict[Dict[str, float]]] = []\n",
        "        for d in self.data:\n",
        "            try:\n",
        "                evaluation_metrics.append(d[\"experiment_metrics\"])\n",
        "            except KeyError:\n",
        "                print(d)\n",
        "\n",
        "        # Flatten the list of dicts\n",
        "        flattened: List[Dict[str, float]] = []\n",
        "        all_metrics = set()\n",
        "        for d in evaluation_metrics:\n",
        "            datum = {}\n",
        "            for component, metrics in d.items():\n",
        "                for metric, value in metrics.items():\n",
        "                    new_metric_name = f\"{component.split('.')[-1]}.{metric}\"\n",
        "                    if type(value) == list:\n",
        "                        value = value[0]\n",
        "                    datum[new_metric_name] = value\n",
        "                    all_metrics.add(new_metric_name)\n",
        "            flattened.append(datum)\n",
        "\n",
        "        if included_metrics:\n",
        "            all_metrics = set(included_metrics)\n",
        "\n",
        "        # Fill in missing metrics\n",
        "        for d in flattened:\n",
        "            for metric in all_metrics:\n",
        "                if metric not in d:\n",
        "                    d[metric] = fill_value\n",
        "\n",
        "            keys = list(d.keys())\n",
        "            for metric in keys:\n",
        "                if metric not in all_metrics:\n",
        "                    del d[metric]\n",
        "\n",
        "        return pd.DataFrame(flattened)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e34e3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "files = os.listdir(experiment_outputs_path)\n",
        "files = [f for f in files if f.endswith(\".jsonl\")]\n",
        "print(files)\n",
        "\n",
        "# Ensure the figures path exists\n",
        "if save_figures:\n",
        "    Path(figures_output_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "data: Dict[str, ExperimentOutput] = {}\n",
        "for file in files:\n",
        "    data[os.path.splitext(file)[0]] = ExperimentOutput(\n",
        "        os.path.join(experiment_outputs_path, file)\n",
        "    )\n",
        "\n",
        "for key, value in data.items():\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    ax = fig.gca()\n",
        "    included_metrics = [\n",
        "        \"components.evaluators.rouge\",\n",
        "        \"components.evaluators.fuzzy\",\n",
        "    ]\n",
        "    value.to_metric_dataframe(included_metrics=included_metrics).hist(\n",
        "        ax=ax, bins=range(0, 101, 5)\n",
        "    )\n",
        "    fig.suptitle(f\"Experiment {key}\", fontsize=12)\n",
        "    fig.tight_layout()\n",
        "    if save_figures:\n",
        "        fig_path = os.path.join(figures_output_path, f\"{key}.png\")\n",
        "        fig.savefig(fig_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5013fd17",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = []\n",
        "included_metrics = [\n",
        "    \"components.evaluators.rouge\",\n",
        "    \"components.evaluators.fuzzy\",\n",
        "]\n",
        "for key, value in data.items():\n",
        "    df = value.to_metric_dataframe(included_metrics=included_metrics)\n",
        "    df[\"experiment\"] = key\n",
        "    dfs.append(df)\n",
        "\n",
        "join = pd.concat(dfs)\n",
        "join.groupby(\"experiment\").mean().head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c3f5f7d86aea6453f6ff1bb883b6cfbdc95927e9a328a4c7bf9548b56af3471"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
