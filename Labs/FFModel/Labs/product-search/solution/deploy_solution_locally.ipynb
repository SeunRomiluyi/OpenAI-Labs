{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Solution Locally\n",
    "\n",
    "This is a light notebook for triggering local solution deployment to manually test out key data points against the LLM-based solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffmodel.core.inference_endpoint import InferenceEndpoint\n",
    "\n",
    "environment_config_path = \"~/.product-search.env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:06:44,051 name=ffmodel.core.inference_endpoint level=INFO Initializing inference runtime. \n",
      "2023-10-05 18:06:44,052 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-05 18:06:44,052 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-05 18:06:44,052 name=ffmodel.core.inference_endpoint level=METRICS solution_config \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 2 configs from file '/Users/jorgeluna/.product-search.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n",
      "FFModelLogger: Logging will print to console as no AppInsight connection was provided (config `APP_INSIGHTS_CONNECTION_STRING)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:06:48,020 name=ffmodel.core.inference_endpoint level=INFO Inference runtime initialized. \n",
      "2023-10-05 18:06:48,020 name=ffmodel.core.inference_endpoint level=INFO Downloading supporting data for solution pipeline. \n",
      "2023-10-05 18:06:48,022 name=ffmodel.core.inference_endpoint level=INFO Downloaded file names: [] \n",
      "2023-10-05 18:06:48,022 name=ffmodel.core.inference_endpoint level=INFO Initializing solution pipeline. \n",
      "2023-10-05 18:06:48,065 name=ffmodel.core.inference_endpoint level=INFO Adding reader components.readers.jsonl to the pipeline. \n",
      "2023-10-05 18:06:48,083 name=ffmodel.core.inference_endpoint level=INFO Adding component components.pre_processors.static_context_from_file to the pipeline. \n",
      "2023-10-05 18:06:48,088 name=ffmodel.core.inference_endpoint level=INFO Adding component components.stitchers.openai_chat_completions to the pipeline. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished pip installing dependencies for project.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:06:48,736 name=ffmodel.core.inference_endpoint level=INFO Adding component components.model_callers.openai_chat_completions to the pipeline. \n",
      "2023-10-05 18:06:48,741 name=ffmodel.core.inference_endpoint level=INFO Skipping non-inference component components.evaluators.semantic_similarity in the pipeline. \n",
      "2023-10-05 18:06:48,745 name=ffmodel.core.inference_endpoint level=INFO Adding writer components.writers.jsonl to the pipeline. \n",
      "2023-10-05 18:06:48,745 name=ffmodel.core.inference_endpoint level=INFO Successfully initialized solution pipeline with solution-id: 01_baseline \n"
     ]
    }
   ],
   "source": [
    "solution_config_path = \"./solution_configs/01_baseline.yaml\"\n",
    "\n",
    "endpoint = InferenceEndpoint(solution_config_path, environment_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 18:06:52,236 name=ffmodel.core.inference_endpoint level=ERROR Failed to parse the request id: None : Extra data: line 1 column 2 (char 1) \n",
      "2023-10-05 18:06:52,237 name=ffmodel.core.inference_endpoint level=ERROR Failed to parse the request id: None : Extra data: line 1 column 2 (char 1) \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jorgeluna/anaconda3/envs/hackpython/lib/python3.11/site-packages/ffmodel/core/inference_endpoint.py\", line 174, in execute\n",
      "    data_model = self._reader.execute(raw_request)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jorgeluna/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/../components/readers/jsonl.py\", line 22, in execute\n",
      "    data_point = json.loads(data)\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jorgeluna/anaconda3/envs/hackpython/lib/python3.11/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jorgeluna/anaconda3/envs/hackpython/lib/python3.11/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 2 (char 1)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to parse the request id: None : Extra data: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/hackpython/lib/python3.11/site-packages/ffmodel/core/inference_endpoint.py:174\u001b[0m, in \u001b[0;36mInferenceEndpoint.execute\u001b[0;34m(self, raw_request, request_id)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     data_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mexecute(raw_request)\n\u001b[1;32m    175\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/../components/readers/jsonl.py:22\u001b[0m, in \u001b[0;36mReader.execute\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mExecutes the reader on a json line data point.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m data_point \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(data)\n\u001b[1;32m     24\u001b[0m data_models \u001b[39m=\u001b[39m create_data_models([data_point], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_model_type)\n",
      "File \u001b[0;32m~/anaconda3/envs/hackpython/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hackpython/lib/python3.11/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jorgeluna/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/deploy_solution_locally.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgeluna/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/deploy_solution_locally.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m user_request \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1,I am planning a camping trip, what do I need to buy?,response\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jorgeluna/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/deploy_solution_locally.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_model \u001b[39m=\u001b[39m endpoint\u001b[39m.\u001b[39;49mexecute(user_request)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jorgeluna/work/OpenAI-Labs/Labs/FFModel/Labs/product-search/solution/deploy_solution_locally.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data_model\u001b[39m.\u001b[39mmodel_output\u001b[39m.\u001b[39mcompletions\n",
      "File \u001b[0;32m~/anaconda3/envs/hackpython/lib/python3.11/site-packages/ffmodel/core/inference_endpoint.py:179\u001b[0m, in \u001b[0;36mInferenceEndpoint.execute\u001b[0;34m(self, raw_request, request_id)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39merror(response_message)\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger\u001b[39m.\u001b[39mexception(response_message)\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response_message)\n\u001b[1;32m    181\u001b[0m \u001b[39m# Create the request metadata\u001b[39;00m\n\u001b[1;32m    182\u001b[0m request_metadata: Dict[\u001b[39mstr\u001b[39m, \u001b[39many\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mException\u001b[0m: Failed to parse the request id: None : Extra data: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"{\"id\":\"4\",\"nl_prompt\":\"What should I buy for a day at Schlitterbahn?\",\"completion\":\"test\"}\"\"\"\n",
    "data_model = endpoint.execute(user_request)\n",
    "data_model.model_output.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.model_output.completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.model_input.prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_model.state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
