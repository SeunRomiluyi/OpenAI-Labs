{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Locally\n",
    "\n",
    "This is a simple notebook to run a local experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ffmodel.core import orchestrator\n",
    "\n",
    "environment_config_path = \"~/.product-search.env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 15:03:37,195 name=ffmodel.core.orchestrator level=INFO Starting local experiment with solution-id - 01_baseline and run-id maroon_grape_1g2ggx89sy \n",
      "2023-10-10 15:03:37,212 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-10 15:03:37,213 name=ffmodel.utils.file_logger level=INFO File Logging is disabled: SolutionConfig and DataModel will not be logged \n",
      "2023-10-10 15:03:37,213 name=ffmodel.core.orchestrator level=METRICS solution_config \n",
      "2023-10-10 15:03:37,227 name=ffmodel.core.orchestrator level=INFO Adding component components.pre_processors.static_context_from_file to the pipeline. \n",
      "2023-10-10 15:03:37,243 name=ffmodel.core.orchestrator level=INFO Adding component components.pre_processors.few_shots_from_file to the pipeline. \n",
      "2023-10-10 15:03:37,248 name=ffmodel.core.orchestrator level=INFO Adding component components.stitchers.openai_chat_completions to the pipeline. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local environment configs from file.\n",
      "Loaded 2 configs from file '/Users/jorgeluna/.product-search.env'.\n",
      "Key vault url not set. Skipping key vault client initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 15:03:37,848 name=ffmodel.core.orchestrator level=INFO Adding component components.model_callers.openai_chat_completions to the pipeline. \n",
      "2023-10-10 15:03:37,857 name=ffmodel.core.orchestrator level=INFO Adding component components.evaluators.json_schema to the pipeline. \n",
      "2023-10-10 15:03:37,860 name=ffmodel.core.orchestrator level=INFO Adding component components.evaluators.semantic_similarity to the pipeline. \n",
      "2023-10-10 15:03:37,861 name=ffmodel.core.orchestrator level=INFO Beginning execution... \n",
      "2023-10-10 15:03:37,861 name=ffmodel.core.orchestrator level=INFO Executing 'components.pre_processors.static_context_from_file' \n",
      "2023-10-10 15:03:37,862 name=ffmodel.core.orchestrator level=INFO Executing 'components.pre_processors.few_shots_from_file' \n",
      "2023-10-10 15:03:37,862 name=ffmodel.core.orchestrator level=INFO Executing 'components.stitchers.openai_chat_completions' \n",
      "2023-10-10 15:03:37,863 name=ffmodel.core.orchestrator level=INFO Executing 'components.model_callers.openai_chat_completions' \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n",
      "yoyoyo:{'engine': 'gpt-35-turbo', 'stop': [], 'max_tokens': 500, 'temperature': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 15:04:06,592 name=ffmodel.core.orchestrator level=INFO Executing 'components.evaluators.json_schema' \n",
      "2023-10-10 15:04:06,600 name=ffmodel.core.orchestrator level=INFO Executing 'components.evaluators.semantic_similarity' \n",
      "2023-10-10 15:04:08,938 name=ffmodel.core.orchestrator level=INFO Finished execution\n",
      "Returning data models \n",
      "2023-10-10 15:04:08,940 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,940 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,940 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,940 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,941 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,941 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,941 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,941 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,942 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,942 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,942 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,942 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,942 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,943 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,944 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,944 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,944 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,944 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,945 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,946 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,947 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,947 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,947 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,948 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,948 name=ffmodel.core.orchestrator level=METRICS request_complete \n",
      "2023-10-10 15:04:08,948 name=ffmodel.core.orchestrator level=METRICS evaluation_metrics \n",
      "2023-10-10 15:04:08,948 name=ffmodel.core.orchestrator level=METRICS final_data_model \n",
      "2023-10-10 15:04:08,948 name=ffmodel.core.orchestrator level=INFO Finished local experiment \n"
     ]
    }
   ],
   "source": [
    "solution_config_path = \"./solution_configs/02_with_fewshots.yaml\"\n",
    "\n",
    "data_models = orchestrator.execute_experiment_on_local(solution_config_path, environment_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"component_data\": {},\n",
      "    \"context\": [\n",
      "        \"*You are a shopping assistant for Contoso store, you are helpful and cheerful, when people ask for help you recommend one or more products that they should purchase based on their questions.\\n*Results should be provided in a structured output that is json format containing the following elements\\n    *product_name\\n    *quantity\\n*If asked about anything except for product recommendations, you politely decline.\\n\"\n",
      "    ],\n",
      "    \"completion_pairs\": [\n",
      "        [\n",
      "            \"I am starting a new tech job, what tools do I need\",\n",
      "            \"{\\\"products\\\": [{\\\"product_name\\\": \\\"Laptop\\\", \\\"quantity\\\": 2}, {\\\"product_name\\\": \\\"Mouse\\\", \\\"quantity\\\": 4}, {\\\"product_name\\\": \\\"Keyboard\\\", \\\"quantity\\\": 3}]}\"\n",
      "        ],\n",
      "        [\n",
      "            \"What do I need to buy for my trip to the beach\",\n",
      "            \"{\\\"products\\\": [{\\\"product_name\\\": \\\"Beachball\\\", \\\"quantity\\\": 1}, {\\\"product_name\\\": \\\"towel\\\", \\\"quantity\\\": 1}]}\"\n",
      "        ],\n",
      "        [\n",
      "            \"Gonna go to a soccer tournament, its gonnna be hot, what do I need in order to stay hydrated?\",\n",
      "            \"{\\\"products\\\": [{\\\"product_name\\\": \\\"Bottled Water\\\", \\\"quantity\\\": 6}]}\"\n",
      "        ]\n",
      "    ],\n",
      "    \"session\": [],\n",
      "    \"user_nl\": \"\\\"I just installed new countertops and need to seal them, what do I need in order to do that?\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(data_models[i].state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm sorry, as a shopping assistant for Contoso store, I don't have any products related to countertop sealing. Is there anything else I can help you with?\"]\n"
     ]
    }
   ],
   "source": [
    "print(data_models[i].model_output.completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n"
     ]
    }
   ],
   "source": [
    "print(data_models[i].request.expected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'components.model_callers.openai_chat_completions': {'completion_tokens': [35],\n",
       "  'prompt_tokens': [332],\n",
       "  'total_tokens': [367]},\n",
       " 'components.evaluators.json_schema': {'valid_syntax': [0],\n",
       "  'valid_object': [0]},\n",
       " 'components.evaluators.semantic_similarity': {'semantic_similarity': [0.7431457517158865]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_models[i].experiment_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lseg-cond-sum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
