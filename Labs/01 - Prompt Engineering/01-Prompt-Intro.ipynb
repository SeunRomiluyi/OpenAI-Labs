{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt \n",
    "Prompt engineering is the developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics (https://www.promptingguide.ai). This notebook provides a basic introduction to creating prompts for ChatGPT language model. A prompt has the following elements:\n",
    "- Instruction\n",
    "- Context\n",
    "- Input Data\n",
    "- Output Indicator\n",
    "\n",
    "Not all elements need to be present in a prompt.\n",
    "\n",
    "## Settings \n",
    "\n",
    "**Temperature** - controls randomness (or creativity), value between 0 and 1, it does this by affecting the probability distribution over the possible tokens at the generation step - a setting of 0 is deterministic, the higher the number the more \"creative\" the response.\n",
    "\n",
    "**Top_p** - Reduces the set of considered tokens to the top percentage, also value between 0 and 1, a setting of 0.1 will consider only the top 10% of the probability mass for the next token.\n",
    "\n",
    "*Low numbers will ensure model provides one right answer.*\n",
    "\n",
    "More Info - (https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/completions#prompt-design) and (https://www.promptingguide.ai/introduction/settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "    \n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Setting up the deployment name\n",
    "chatgpt_model_name = os.getenv(\"CHATGPT_MODEL\")\n",
    "\n",
    "# Currently Chat Completions API have the following versions available: 2023-03-15-preview\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Define helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "# More info : https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions\n",
    "def send_prompt(prompt, temperature=0.5, top_p=0.5, max_response_tokens=500):\n",
    "    messages = [{\"role\": \"system\", \"content\": \"Assistant is an intelligent chatbot that helps users do just about anything. \"}]\n",
    "    messages.append({\"role\":\"user\",\"content\": prompt})\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=chatgpt_model_name,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "def print_response(response):\n",
    "    print('Response: ', response,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Simple Prompt - No instruction\n",
    "Example of a very simple prompt so we can see what the model returns for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "The sky is\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Run with different settings\n",
    "\n",
    "In these examples we will run the single prompt using different settings for Temperature and Top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "What is the largest truck for towing\n",
    "\"\"\"\n",
    "response = send_prompt(prompt,0,0)\n",
    "print_response(response)\n",
    "\n",
    "response = send_prompt(prompt,0.5,0.5)\n",
    "print_response(response)\n",
    "\n",
    "response = send_prompt(prompt,0,1)\n",
    "print_response(response)\n",
    "\n",
    "response = send_prompt(prompt,1,0)\n",
    "print_response(response)\n",
    "\n",
    "response = send_prompt(prompt,1,1)\n",
    "print_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Prompt with Instructions\n",
    "These examples provide instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "Finish the following sentence and print the entire resulting sentence:\n",
    "The sky is\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print(response)\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Translate the text below to Spanish:\n",
    "Text: \"hello!\"\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print(response)\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Explain lacrosse\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Prompt with Context\n",
    "These examples provide instructions in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =  \"\"\"\n",
    "Question: What was OKT3 originally sourced from?\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print(response)\n",
    "\n",
    "prompt =  \"\"\"\n",
    "Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
    "Question: What was OKT3 originally sourced from?\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = send_prompt(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
